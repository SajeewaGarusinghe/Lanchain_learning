{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Data Ingestion with Document Loaders\n",
    "\n",
    "## What is Data Ingestion?\n",
    "\n",
    "**Data Ingestion** is the process of loading data from various sources into a format that LangChain can work with. Think of it as the \"import\" step - getting your data ready for processing.\n",
    "\n",
    "## What are Document Loaders?\n",
    "\n",
    "Document Loaders are LangChain's way of:\n",
    "- üìÑ Reading data from different sources (files, web, APIs)\n",
    "- üîÑ Converting that data into a standardized `Document` format\n",
    "- üìã Preserving **metadata** (like source, page number, etc.)\n",
    "\n",
    "### The Document Object\n",
    "\n",
    "Every loader returns a list of `Document` objects with two main attributes:\n",
    "- `page_content`: The actual text content\n",
    "- `metadata`: Information about the source (filename, page number, URL, etc.)\n",
    "\n",
    "### Common Document Loaders\n",
    "\n",
    "| Loader | Use Case |\n",
    "|--------|----------|\n",
    "| `TextLoader` | Plain text files (.txt) |\n",
    "| `PyPDFLoader` | PDF documents |\n",
    "| `WebBaseLoader` | Web pages |\n",
    "| `CSVLoader` | CSV files |\n",
    "| `ArxivLoader` | Academic papers from Arxiv |\n",
    "| `WikipediaLoader` | Wikipedia articles |\n",
    "\n",
    "üìñ **Full List**: https://python.langchain.com/v0.2/docs/integrations/document_loaders/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ TextLoader - Loading Plain Text Files\n",
    "\n",
    "The simplest document loader - reads `.txt` files directly.\n",
    "\n",
    "**Use Cases:**\n",
    "- Reading transcripts\n",
    "- Loading plain text documents\n",
    "- Processing text exports\n",
    "\n",
    "**Key Points:**\n",
    "- Returns a single Document for the entire file\n",
    "- Preserves the source path in metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader type: <class 'langchain_community.document_loaders.text.TextLoader'>\n",
      "File path: speech.txt\n"
     ]
    }
   ],
   "source": [
    "# TextLoader - For loading plain .txt files\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Step 1: Create a loader instance by specifying the file path\n",
    "loader = TextLoader('speech.txt')\n",
    "\n",
    "# Step 2: The loader object is ready but hasn't loaded the data yet\n",
    "print(f\"Loader type: {type(loader)}\")\n",
    "print(f\"File path: {loader.file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Document type: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "--- Metadata ---\n",
      "{'source': 'speech.txt'}\n",
      "\n",
      "--- Content Preview (first 500 chars) ---\n",
      "The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\n",
      "\n",
      "Just because we fight withou\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Call .load() to actually read the file and get Document objects\n",
    "text_documents = loader.load()\n",
    "\n",
    "# Let's explore what we got\n",
    "print(f\"Number of documents: {len(text_documents)}\")\n",
    "print(f\"Document type: {type(text_documents[0])}\")\n",
    "print(f\"\\n--- Metadata ---\")\n",
    "print(text_documents[0].metadata)\n",
    "print(f\"\\n--- Content Preview (first 500 chars) ---\")\n",
    "print(text_documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ PyPDFLoader - Loading PDF Documents\n",
    "\n",
    "PDF files are everywhere! PyPDFLoader extracts text from PDF files.\n",
    "\n",
    "**Key Features:**\n",
    "- üìÑ Creates **one Document per page** (great for large PDFs!)\n",
    "- üìä Includes page number in metadata\n",
    "- ‚ö° Uses `pypdf` library under the hood\n",
    "\n",
    "**Installation:** `pip install pypdf`\n",
    "\n",
    "**Use Cases:**\n",
    "- Research papers\n",
    "- Reports and documentation\n",
    "- Any PDF content you need to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages/documents: 15\n",
      "Document type: <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "# PyPDFLoader - For loading PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load a PDF file (the famous \"Attention Is All You Need\" paper)\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "\n",
    "# Load returns one Document per page!\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Total pages/documents: {len(docs)}\")\n",
    "print(f\"Document type: {type(docs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Document attributes:\n",
      "  - page_content: The actual text\n",
      "  - metadata: Information about the source\n"
     ]
    }
   ],
   "source": [
    "# Check the type - it's a LangChain Document object\n",
    "print(f\"Type: {type(docs[0])}\")\n",
    "print(f\"\\nDocument attributes:\")\n",
    "print(f\"  - page_content: The actual text\")\n",
    "print(f\"  - metadata: Information about the source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== METADATA ===\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "=== PAGE CONTENT (first 800 characters) ===\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó ‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó ‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder an\n"
     ]
    }
   ],
   "source": [
    "# Explore the first page (index 0)\n",
    "first_page = docs[0]\n",
    "\n",
    "print(\"=== METADATA ===\")\n",
    "print(first_page.metadata)\n",
    "\n",
    "print(\"\\n=== PAGE CONTENT (first 800 characters) ===\")\n",
    "print(first_page.page_content[:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ WebBaseLoader - Loading Web Pages\n",
    "\n",
    "Scrape content directly from websites! Uses BeautifulSoup under the hood.\n",
    "\n",
    "**Key Features:**\n",
    "- üåê Load content from any URL\n",
    "- üéØ Filter specific HTML elements using BeautifulSoup\n",
    "- üì¶ Can load multiple URLs at once\n",
    "\n",
    "**Installation:** `pip install beautifulsoup4`\n",
    "\n",
    "**Use Cases:**\n",
    "- Blog posts and articles\n",
    "- Documentation pages\n",
    "- News articles\n",
    "- Any web content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# WebBaseLoader - Basic usage (loads entire page)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Load a blog post about AI Agents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",)\n",
    ")\n",
    "\n",
    "# Note: web_paths takes a tuple, even for single URLs (notice the comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "\n",
      "=== METADATA ===\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}\n",
      "\n",
      "=== CONTENT PREVIEW (first 1000 chars) ===\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the web page content\n",
    "web_docs = loader.load()\n",
    "\n",
    "print(f\"Number of documents: {len(web_docs)}\")\n",
    "print(f\"\\n=== METADATA ===\")\n",
    "print(web_docs[0].metadata)\n",
    "print(f\"\\n=== CONTENT PREVIEW (first 1000 chars) ===\")\n",
    "print(web_docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Web Content with BeautifulSoup\n",
    "\n",
    "Often, you don't want the entire page (navigation, ads, footers, etc.). \n",
    "Use `bs_kwargs` to filter specific HTML elements!\n",
    "\n",
    "**SoupStrainer** lets you specify which elements to extract:\n",
    "- `class_`: Filter by CSS class names\n",
    "- `id`: Filter by element IDs\n",
    "- Other HTML attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebBaseLoader with BeautifulSoup filtering\n",
    "# This extracts ONLY the main content (title, header, body)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            # Only extract elements with these CSS classes\n",
    "            class_=(\"post-title\", \"post-content\", \"post-header\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# This gives us cleaner content without navigation/footer clutter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "\n",
      "=== CLEANER CONTENT PREVIEW (first 1500 chars) ===\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information\n"
     ]
    }
   ],
   "source": [
    "# Load the filtered content\n",
    "filtered_docs = loader.load()\n",
    "\n",
    "print(f\"Number of documents: {len(filtered_docs)}\")\n",
    "print(f\"\\n=== CLEANER CONTENT PREVIEW (first 1500 chars) ===\")\n",
    "print(filtered_docs[0].page_content[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ ArxivLoader - Loading Academic Papers\n",
    "\n",
    "Access academic papers directly from [arXiv](https://arxiv.org/)!\n",
    "\n",
    "**Key Features:**\n",
    "- üî¨ Load papers by arXiv ID (e.g., \"1706.03762\")\n",
    "- üîç Search papers by topic/query\n",
    "- üìä Rich metadata (authors, title, abstract, etc.)\n",
    "- üìñ Extracts full PDF text\n",
    "\n",
    "**Installation:** `pip install arxiv pymupdf`\n",
    "\n",
    "**Use Cases:**\n",
    "- Research assistance\n",
    "- Academic Q&A systems\n",
    "- Paper summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n"
     ]
    }
   ],
   "source": [
    "# ArxivLoader - Load academic papers from arXiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Load the famous \"Attention Is All You Need\" paper\n",
    "# \"1706.03762\" is the arXiv ID\n",
    "docs = ArxivLoader(\n",
    "    query=\"1706.03762\",  # Can be arXiv ID or search query\n",
    "    load_max_docs=2       # Limit number of papers to load\n",
    ").load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PAPER METADATA ===\n",
      "  Published: 2023-08-02\n",
      "  Title: Attention Is All You Need\n",
      "  Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kais...\n",
      "  Summary: The dominant sequence transduction models are based on complex recurrent or convolutional neural net...\n",
      "\n",
      "=== CONTENT PREVIEW (first 1000 chars) ===\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Exper\n"
     ]
    }
   ],
   "source": [
    "# Explore the loaded paper\n",
    "print(\"=== PAPER METADATA ===\")\n",
    "for key, value in docs[0].metadata.items():\n",
    "    # Truncate long values for display\n",
    "    if isinstance(value, str) and len(value) > 100:\n",
    "        value = value[:100] + \"...\"\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n=== CONTENT PREVIEW (first 1000 chars) ===\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ WikipediaLoader - Loading Wikipedia Articles\n",
    "\n",
    "Access Wikipedia's vast knowledge base directly!\n",
    "\n",
    "**Key Features:**\n",
    "- üîç Search Wikipedia by topic\n",
    "- üìö Returns article summaries and content\n",
    "- üåç Supports multiple languages\n",
    "- üìã Rich metadata\n",
    "\n",
    "**Installation:** `pip install wikipedia`\n",
    "\n",
    "**Use Cases:**\n",
    "- General knowledge Q&A\n",
    "- Research and fact-checking\n",
    "- Building knowledge bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles loaded: 2\n"
     ]
    }
   ],
   "source": [
    "# WikipediaLoader - Search and load Wikipedia articles\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# Search for articles about \"Generative AI\"\n",
    "docs = WikipediaLoader(\n",
    "    query=\"Generative AI\",  # Your search query\n",
    "    load_max_docs=2          # Number of articles to load\n",
    ").load()\n",
    "\n",
    "print(f\"Number of articles loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìÑ Article 1\n",
      "==================================================\n",
      "Title: Generative artificial intelligence\n",
      "Source: https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n",
      "\n",
      "Content Preview (first 500 chars):\n",
      "Generative artificial intelligence (Generative AI, or GenAI) is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data in response to input, which often comes in the form of natural language prompts.\n",
      "The prevalence of generative AI tools has increased significantly since the AI boom in the 2020s\n",
      "...\n",
      "\n",
      "==================================================\n",
      "üìÑ Article 2\n",
      "==================================================\n",
      "Title: Generative AI pornography\n",
      "Source: https://en.wikipedia.org/wiki/Generative_AI_pornography\n",
      "\n",
      "Content Preview (first 500 chars):\n",
      "Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\n",
      "\n",
      "\n",
      "== Functions and production strate\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Explore Wikipedia documents\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìÑ Article {i+1}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"\\nContent Preview (first 500 chars):\")\n",
    "    print(doc.page_content[:500])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Common installations for Document Loaders:\n",
      "   pip install langchain-community\n",
      "   pip install pypdf beautifulsoup4 arxiv pymupdf wikipedia\n"
     ]
    }
   ],
   "source": [
    "# üéØ Quick Reference: Install required packages\n",
    "# Run this cell if you need to install the dependencies\n",
    "\n",
    "# !pip install langchain-community\n",
    "# !pip install pypdf          # For PyPDFLoader\n",
    "# !pip install beautifulsoup4 # For WebBaseLoader\n",
    "# !pip install arxiv pymupdf  # For ArxivLoader\n",
    "# !pip install wikipedia      # For WikipediaLoader\n",
    "\n",
    "print(\"üì¶ Common installations for Document Loaders:\")\n",
    "print(\"   pip install langchain-community\")\n",
    "print(\"   pip install pypdf beautifulsoup4 arxiv pymupdf wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary: Key Takeaways\n",
    "\n",
    "### Document Loaders Overview\n",
    "\n",
    "| Loader | Source | Documents Per |\n",
    "|--------|--------|---------------|\n",
    "| `TextLoader` | .txt files | 1 per file |\n",
    "| `PyPDFLoader` | PDF files | 1 per page |\n",
    "| `WebBaseLoader` | Web URLs | 1 per URL |\n",
    "| `CSVLoader` | CSV files | 1 per row |\n",
    "| `ArxivLoader` | arXiv papers | 1 per paper |\n",
    "| `WikipediaLoader` | Wikipedia | 1 per article |\n",
    "| `DirectoryLoader` | Folders | Depends on file type |\n",
    "\n",
    "### Common Pattern\n",
    "\n",
    "```python\n",
    "# All loaders follow this pattern:\n",
    "from langchain_community.document_loaders import SomeLoader\n",
    "\n",
    "# 1. Create loader instance\n",
    "loader = SomeLoader(source=\"path/url/query\")\n",
    "\n",
    "# 2. Load documents\n",
    "docs = loader.load()\n",
    "\n",
    "# 3. Access content and metadata\n",
    "for doc in docs:\n",
    "    print(doc.page_content)  # The text\n",
    "    print(doc.metadata)       # Source info\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "- üî™ **Text Splitting**: Break large documents into smaller chunks\n",
    "- üßÆ **Embeddings**: Convert text to vectors for similarity search\n",
    "- üíæ **Vector Stores**: Store and retrieve documents efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirectoryLoader patterns (glob):\n",
      "  '*.txt'      - All .txt files in the directory\n",
      "  '**/*.txt'   - All .txt files (recursive)\n",
      "  '**/*.pdf'   - All .pdf files (recursive)\n",
      "  '**/*.*'     - All files (recursive)\n"
     ]
    }
   ],
   "source": [
    "# DirectoryLoader - Load all files from a folder\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Example: Load all .txt files from a directory\n",
    "# loader = DirectoryLoader(\n",
    "#     path=\"./documents/\",          # Directory path\n",
    "#     glob=\"**/*.txt\",               # Pattern: all .txt files (recursive)\n",
    "#     loader_cls=TextLoader,         # Which loader to use\n",
    "#     show_progress=True             # Show loading progress\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "print(\"DirectoryLoader patterns (glob):\")\n",
    "print(\"  '*.txt'      - All .txt files in the directory\")\n",
    "print(\"  '**/*.txt'   - All .txt files (recursive)\")\n",
    "print(\"  '**/*.pdf'   - All .pdf files (recursive)\")\n",
    "print(\"  '**/*.*'     - All files (recursive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ DirectoryLoader - Loading Multiple Files\n",
    "\n",
    "Load all files from a directory at once!\n",
    "\n",
    "**Key Features:**\n",
    "- üìÅ Batch load entire directories\n",
    "- üéØ Filter by file extension (glob patterns)\n",
    "- üîÑ Choose loader type for each file type\n",
    "- ‚ö° Supports parallel loading\n",
    "\n",
    "**Use Cases:**\n",
    "- Loading entire documentation folders\n",
    "- Processing multiple reports\n",
    "- Bulk data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVLoader creates one Document per row in your CSV file!\n",
      "Each row's content is formatted as 'column: value' pairs\n"
     ]
    }
   ],
   "source": [
    "# CSVLoader - For loading CSV/spreadsheet data\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Example: Load a CSV file (uncomment when you have a CSV file)\n",
    "# loader = CSVLoader(\n",
    "#     file_path=\"your_data.csv\",\n",
    "#     csv_args={\n",
    "#         'delimiter': ',',\n",
    "#         'quotechar': '\"'\n",
    "#     }\n",
    "# )\n",
    "# docs = loader.load()\n",
    "\n",
    "# Each row becomes a Document!\n",
    "# for doc in docs[:3]:\n",
    "#     print(doc.page_content)\n",
    "#     print(doc.metadata)\n",
    "#     print(\"---\")\n",
    "\n",
    "print(\"CSVLoader creates one Document per row in your CSV file!\")\n",
    "print(\"Each row's content is formatted as 'column: value' pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ CSVLoader - Loading CSV Files\n",
    "\n",
    "Perfect for structured data in spreadsheets!\n",
    "\n",
    "**Key Features:**\n",
    "- üìä Each row becomes a separate Document\n",
    "- üè∑Ô∏è Column values become metadata\n",
    "- ‚öôÔ∏è Customizable field mappings\n",
    "\n",
    "**Use Cases:**\n",
    "- Customer data\n",
    "- Product catalogs\n",
    "- FAQ databases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
