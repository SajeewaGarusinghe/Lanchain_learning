{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”ª RecursiveCharacterTextSplitter - The Smart Text Splitter\n",
    "\n",
    "## What is Text Splitting?\n",
    "\n",
    "After loading documents, they're often **too large** for LLMs to process effectively. Text splitting breaks them into smaller, manageable **chunks**.\n",
    "\n",
    "## Why RecursiveCharacterTextSplitter? ğŸŒŸ\n",
    "\n",
    "This is the **#1 recommended splitter** for generic text! Here's why:\n",
    "\n",
    "### How It Works:\n",
    "1. Tries to split on `\"\\n\\n\"` (paragraphs) first\n",
    "2. If chunks are still too big â†’ splits on `\"\\n\"` (lines)\n",
    "3. If still too big â†’ splits on `\" \"` (words)\n",
    "4. Last resort â†’ splits on `\"\"` (characters)\n",
    "\n",
    "### Key Parameters:\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| `chunk_size` | Maximum size of each chunk | 500 characters |\n",
    "| `chunk_overlap` | Characters shared between chunks | 50 characters |\n",
    "| `separators` | List of separators to try | `[\"\\n\\n\", \"\\n\", \" \", \"\"]` |\n",
    "\n",
    "### Why Overlap? ğŸ”„\n",
    "Overlap ensures **context is preserved** between chunks. Without it, sentences might be cut mid-way, losing meaning.\n",
    "\n",
    "```\n",
    "Chunk 1: [........text........]\n",
    "Chunk 2:              [........text........]\n",
    "                      â†‘ overlap area â†‘\n",
    "```\n",
    "\n",
    "ğŸ“– **Docs**: https://python.langchain.com/docs/how_to/recursive_text_splitter/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Splitting PDF Documents\n",
    "\n",
    "Let's start by loading a PDF and then splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loaded 15 pages from PDF\n",
      "ğŸ“ First page length: 2859 characters\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load a PDF document first\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the \"Attention Is All You Need\" paper\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ğŸ“„ Loaded {len(docs)} pages from PDF\")\n",
    "print(f\"ğŸ“ First page length: {len(docs[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Document has two attributes:\n",
      "  â€¢ page_content: The actual text\n",
      "  â€¢ metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Each page is a Document object\n",
    "print(f\"Type: {type(docs[0])}\")\n",
    "print(f\"\\nDocument has two attributes:\")\n",
    "print(f\"  â€¢ page_content: The actual text\")\n",
    "print(f\"  â€¢ metadata: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Using RecursiveCharacterTextSplitter\n",
    "\n",
    "Now let's split those PDF pages into smaller chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Original: 15 pages\n",
      "ğŸ“Š After splitting: 93 chunks\n",
      "ğŸ“Š Average expansion: 6.2x\n"
     ]
    }
   ],
   "source": [
    "# Import the text splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create splitter with custom settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # Each chunk max 500 characters\n",
    "    chunk_overlap=50     # 50 character overlap between chunks\n",
    ")\n",
    "\n",
    "# Split the documents - this works directly with Document objects!\n",
    "final_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"ğŸ“Š Original: {len(docs)} pages\")\n",
    "print(f\"ğŸ“Š After splitting: {len(final_documents)} chunks\")\n",
    "print(f\"ğŸ“Š Average expansion: {len(final_documents)/len(docs):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ“„ CHUNK 1:\n",
      "==================================================\n",
      "Length: 492 chars\n",
      "Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswaniâˆ—\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeerâˆ—\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n",
      "\n",
      "==================================================\n",
      "ğŸ“„ CHUNK 2:\n",
      "==================================================\n",
      "Length: 454 chars\n",
      "Content: University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Åukasz Kaiserâˆ—\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhinâˆ— â€¡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The ...\n"
     ]
    }
   ],
   "source": [
    "# Compare first two chunks - notice they're smaller and may overlap\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“„ CHUNK 1:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Length: {len(final_documents[0].page_content)} chars\")\n",
    "print(f\"Content: {final_documents[0].page_content[:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“„ CHUNK 2:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Length: {len(final_documents[1].page_content)} chars\")\n",
    "print(f\"Content: {final_documents[1].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Splitting Plain Text Files\n",
    "\n",
    "You can also split raw text strings using `create_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loaded document\n",
      "ğŸ“ Total length: 3624 characters\n",
      "\n",
      "--- Preview ---\n",
      "The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\n",
      "\n",
      "Just because we fight withou\n"
     ]
    }
   ],
   "source": [
    "# Load a text file using TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ğŸ“„ Loaded document\")\n",
    "print(f\"ğŸ“ Total length: {len(docs[0].page_content)} characters\")\n",
    "print(f\"\\n--- Preview ---\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Created 49 chunks from speech\n",
      "\n",
      "--- First Chunk ---\n",
      "Length: 99 chars\n",
      "The world must be made safe for democracy. Its peace must be planted upon the tested foundations of\n",
      "\n",
      "--- Second Chunk ---\n",
      "Length: 93 chars\n",
      "foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Read file directly and use create_documents()\n",
    "# This is useful when you have raw strings instead of Document objects\n",
    "\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech = f.read()\n",
    "\n",
    "# Create smaller chunks from raw text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,      # Smaller chunks for demo\n",
    "    chunk_overlap=20     # 20 char overlap\n",
    ")\n",
    "\n",
    "# create_documents() takes a list of strings and returns Document objects\n",
    "text_chunks = text_splitter.create_documents([speech])\n",
    "\n",
    "print(f\"ğŸ“Š Created {len(text_chunks)} chunks from speech\")\n",
    "print(f\"\\n--- First Chunk ---\")\n",
    "print(f\"Length: {len(text_chunks[0].page_content)} chars\")\n",
    "print(text_chunks[0].page_content)\n",
    "\n",
    "print(f\"\\n--- Second Chunk ---\")\n",
    "print(f\"Length: {len(text_chunks[1].page_content)} chars\")\n",
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "You can access:\n",
      "  â€¢ .page_content - the text\n",
      "  â€¢ .metadata - source info (empty for create_documents by default)\n"
     ]
    }
   ],
   "source": [
    "# The result is still a Document object!\n",
    "print(f\"Type: {type(text_chunks[0])}\")\n",
    "print(f\"\\nYou can access:\")\n",
    "print(f\"  â€¢ .page_content - the text\")\n",
    "print(f\"  â€¢ .metadata - source info (empty for create_documents by default)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### Two Ways to Split Text:\n",
    "\n",
    "| Method | Input | Use When |\n",
    "|--------|-------|----------|\n",
    "| `split_documents(docs)` | List of Document objects | After using a DocumentLoader |\n",
    "| `create_documents([strings])` | List of raw strings | When you have plain text |\n",
    "\n",
    "### Choosing chunk_size and chunk_overlap:\n",
    "\n",
    "- **chunk_size**: Depends on your LLM's context window\n",
    "  - Smaller (100-500): More precise retrieval, less context\n",
    "  - Larger (500-2000): More context, may include irrelevant info\n",
    "\n",
    "- **chunk_overlap**: Usually 10-20% of chunk_size\n",
    "  - Too small: May lose context at boundaries\n",
    "  - Too large: Redundant information, more chunks\n",
    "\n",
    "### Best Practices ğŸ’¡\n",
    "\n",
    "1. âœ… Use RecursiveCharacterTextSplitter for general text\n",
    "2. âœ… Test different chunk sizes for your use case\n",
    "3. âœ… Keep overlap for context preservation\n",
    "4. âœ… Consider your embedding model's token limits\n",
    "\n",
    "### Next Steps ğŸš€\n",
    "- Learn about **CharacterTextSplitter** for simple splitting\n",
    "- Explore **HTMLTextSplitter** for web content\n",
    "- Try **RecursiveJsonSplitter** for JSON data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
